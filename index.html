<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>jimfund</title>
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400;0,500;1,400&display=swap" rel="stylesheet">
        <style>
            :root {
                --bg-color: #fafafa;
                --text-color: #222;
                --text-muted: #888;
                --text-hover: #555;
                --link-color: #222;
                --blockquote-border: #e0e0e0;
                --blockquote-text: #555;
                --shadow-color: rgba(0,0,0,0.1);
            }

            .dark-mode {
                --bg-color: #1a1a1a;
                --text-color: #e0e0e0;
                --text-muted: #888;
                --text-hover: #bbb;
                --link-color: #e0e0e0;
                --blockquote-border: #444;
                --blockquote-text: #aaa;
                --shadow-color: rgba(0,0,0,0.4);
            }

            * {
                margin: 0;
                padding: 0;
                box-sizing: border-box;
            }

            .article-view .content img {
                max-width: 100%;
                height: auto;
                display: block;
                margin: 1.5rem 0;
                border-radius: 8px;
                box-shadow: 0 4px 12px var(--shadow-color);
            }

            .article-view .content blockquote {
                margin: 1.5rem 0 1.5rem 0;
                padding-left: 1.25rem;
                border-left: 4px solid var(--blockquote-border);
                color: var(--blockquote-text);
                font-style: italic;
            }

            body {
                font-family: 'EB Garamond', Georgia, serif;
                font-size: 18px;
                line-height: 1.6;
                color: var(--text-color);
                background: var(--bg-color);
                padding: 3rem 1.5rem;
                max-width: 600px;
                margin: 0 auto;
                transition: background-color 0.3s ease, color 0.3s ease;
            }

            header {
                margin-bottom: 3rem;
                display: flex;
                justify-content: space-between;
                align-items: center;
            }

            h1 {
                font-size: 1.5rem;
                font-weight: 500;
                letter-spacing: -0.02em;
            }

            h1 a {
                color: inherit;
                text-decoration: none;
            }

            h1 a:hover {
                color: var(--text-hover);
            }

            .theme-toggle {
                background: none;
                border: none;
                cursor: pointer;
                padding: 0.25rem;
                color: var(--text-muted);
                transition: color 0.2s ease;
                display: flex;
                align-items: center;
            }

            .theme-toggle:hover {
                color: var(--text-color);
            }

            .theme-toggle .sun {
                display: none;
            }

            .theme-toggle .moon {
                display: block;
            }

            .dark-mode .theme-toggle .sun {
                display: block;
            }

            .dark-mode .theme-toggle .moon {
                display: none;
            }

            .articles {
                list-style: none;
            }

            .article {
                margin-bottom: 0.75rem;
                display: flex;
                gap: 1.5rem;
                align-items: baseline;
            }

            .date {
                color: var(--text-muted);
                font-size: 0.9rem;
                flex-shrink: 0;
                width: 6rem;
            }

            .title {
                color: var(--text-color);
                text-decoration: none;
                cursor: pointer;
            }

            .title:hover {
                color: var(--text-hover);
            }

            .article-view {
                display: none;
            }

            .article-view.active {
                display: block;
            }

            .article-view .article-date {
                color: var(--text-muted);
                font-size: 0.9rem;
                margin-bottom: 1.5rem;
            }

            .article-view h2 {
                font-size: 1.75rem;
                font-weight: 500;
                margin-bottom: 0.5rem;
                letter-spacing: -0.02em;
            }

            .article-view .content {
                margin-top: 2rem;
            }

            .article-view .content p {
                margin-bottom: 1.25rem;
            }

            .article-view .content a {
                color: var(--link-color);
                text-decoration: underline;
            }

            .article-view .content a:hover {
                color: var(--text-hover);
            }

            .home-view.hidden {
                display: none;
            }

            .article-view .content ul {
                margin-bottom: 1.25rem;
                padding-left: 1.5rem;
                list-style-type: disc;
            }

            .article-view .content li {
                margin-bottom: 0.5rem;
                padding-left: 0.25rem;
            }

            .article-view .content ul ul {
                margin-bottom: 0;
                margin-top: 0.5rem;
                list-style-type: circle;
            }

            @media (max-width: 500px) {
                .article {
                    flex-direction: column;
                    gap: 0.1rem;
                    margin-bottom: 1.25rem;
                }

                .date {
                    width: auto;
                }
            }
        </style>
    </head>
    <body>
        <header>
            <h1><a href="#" onclick="showHome(); return false;">jimfund</a></h1>
            <button class="theme-toggle" onclick="toggleTheme()" aria-label="Toggle dark mode">
                <svg class="sun" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <circle cx="12" cy="12" r="5"></circle>
                    <line x1="12" y1="1" x2="12" y2="3"></line>
                    <line x1="12" y1="21" x2="12" y2="23"></line>
                    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                    <line x1="1" y1="12" x2="3" y2="12"></line>
                    <line x1="21" y1="12" x2="23" y2="12"></line>
                    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                </svg>
                <svg class="moon" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                </svg>
            </button>
        </header>

        <main>
            <div class="home-view">
                <ul class="articles">
                    
                    <li class="article">
                        <span class="date">08 Mar 2025</span>
                        <a href="#" class="title" onclick="showArticle('mind-upload'); return false;">Mind Upload</a>
                    </li>
                    
                    <li class="article">
                        <span class="date">06 Mar 2025</span>
                        <a href="#" class="title" onclick="showArticle('math'); return false;">Scaling Inference-Time Reasoning Will Enable Fully-Autonomous Mathematics Researchers</a>
                    </li>
                    
                    <li class="article">
                        <span class="date">10 Oct 2024</span>
                        <a href="#" class="title" onclick="showArticle('near-future-fiction'); return false;">Near-Future Fiction</a>
                    </li>
                    
                </ul>
            </div>

            
            <article class="article-view" id="mind-upload">
                <h2>Mind Upload</h2>
                <p class="article-date">08 Mar 2025</p>
                <div class="content">
                    <p>Once aligned ASI is achieved, it will invent the technology to create digital copies of people. This is mind upload. The worlds people will inhabit once uploaded will be personal utopias curated by artificial superintelligence. What I mean by “personal utopias” is that these will be worlds created specifically for individual uploaded minds, optimised for their personal flourishing. They will not primarily contain utopian societies. It’s the best-possible world for the uploaded mind, but not for the other inhabitants of the world.</p>

<p>This would be ethically questionable if the other inhabitants were moral patients. So, they won’t be. They will be P-zombies, agents indistinguishable by any empirical means from natural humans, but who lack the light of consciousness, or any subjective experience.</p>

<p>One might object. “But would one, living in such a world, not feel unsatisfied in one’s relationships and interactions with the other inhabitants, knowing that they did not really perceive one, or feel anything about one at all?” I think this is a point which needs addressing. The claim seems true, that knowing one is the only truly conscious person in the world would for most people reduce the satisfaction of existence in that world. It could still be enjoyable; one enjoys video games. But, despite it being enjoyable, it seems unlikely that it would be the best possible world for one. The solution is simple enough. Just have one’s ASI world-curator remove from one’s mind the awareness of the fact that the other occupants in the world are P-zombies.</p>

<p>There is a loss in the erasure: that one will feel the moral weight of one’s actions, which will cause certain inhibitions. But this would not be in balance a loss, but, rather, a gain, as life would feel ultimately purposeless otherwise.</p>

<p>Another objection is that people would not readily abandon the friends they had pre-upload. I do not deny this. Humans are emotional creatures. And it will be possible to co-inhabit digital environments with one’s pre-upload friends. But people will soon choose to splinter off into the kinds of personal utopias I outlined above.</p>

<p>People will quickly find themselves forming much stronger, deeper connections with artificial people than they ever did with natural people (why is explained later in this post). And as people find themselves spending ~no time with those whom they had previously held dear, sharing a world with them becomes strictly costly: instead of constructing a world which is the best possible world for one, the ASI world-curator must compromise between what is best for one and what is best for one’s friends and loved-ones. And to whatever extent one’s actions are legible to one’s pre-upload friends, one is inhibited in one’s inevitable wish not to incur judgement for violating the ethical norms of pre-upload society, which will generally be far from the norms which bring an individual the greatest good. Therefore, people will splinter off into their own worlds, isolated from other humans.</p>

<p>So, I have established some of the parameters of the worlds which our uploaded minds will inhabit. They will be worlds curated by ASI to be the best possible worlds for their single conscious inhabitants. The ASI will have general freedom in shaping the world, unburdened by ethical considerations beyond those which concern these individuals. But, concretely, what will these personal virtual utopias actually be like?</p>

<p>As I mentioned earlier, they will be very different to any traditional depictions of utopian society. After all, utopia as popularly conceived is paradoxical: it attempts to solve for a society that simultaneously grants purpose and freedom but also abundance and peace. But each side is only really attainable at the cost of the other. ASI curators will resolve the paradox by focusing on purpose and freedom and giving up on abundance and peace. Abundance and peace are societal goals, but not fundamentally important to the individual’s good. For the individual, struggle, danger, pain, and self-sacrifice are all aspects of a good life. A world of high-stakes, where things are bad and need to be changed, and evil forces need to be repelled, and there is much that is unknown&hellip; it’s not a world of abundance and peace, but it’s the world one would likely like to live in.</p>

<p>But all of that is still abstract, and we’re trying to get a more concrete picture of life in these personal utopias. Well, the world is meant to be the best possible world for one. So imagine all the things which have brought your joy in this world. In one’s personal utopia, amplified versions of all these joys will be present. One will witness events more interesting than any one witnessed pre-upload, make stronger emotional connections, accomplish greater things, experience deeper love, stronger passion, take bigger risks, experience greater turns of fortune, etc.</p>

<p>People have a natural tendency to try to come up with ways in which such a world would be worse than the real world, rather than better. But it wouldn’t be, at least not from one’s subjective perspective. The only way in which it would be worse is that one’s actions would not be truly meaningful. Today, in the real world, one’s actions influence (we presume) such things as whether safe ASI will really be developed, which determines the fate of at least billions of souls. In our virtual utopias, though we will not know it, our actions will not be truly meaningful. But, nonetheless, subjectively they will likely be strictly better than the real world, and definitely be, subjectively, broadly, vastly preferable.</p>

<p>So, I imagine virtual personal utopian worlds as being places of righteous martyrs; grand betrayals; convoluted plots; ancient families; galactic empires; deep magic; inexhaustible lore; perfectly-written characters of all moral colours: good, evil, grey, with moral arcs from good to evil (and vice versa), etc.; worlds full of diverse civilisations, immense beauty, and so on. But, more than any of that, worlds in which the main characters live lives of grand struggle and triumph, loss and discovery, etc.</p>
                </div>
            </article>
            
            <article class="article-view" id="math">
                <h2>Scaling Inference-Time Reasoning Will Enable Fully-Autonomous Mathematics Researchers</h2>
                <p class="article-date">06 Mar 2025</p>
                <div class="content">
                    <p>A fully-autonomous mathematics researcher must do two things. It must solve open mathematics problems. And it must pose interesting mathematics problems within its reach to solve. I will argue that reasoning will scale to enable these two things.</p>

<h2>Solving Open Math Problems</h2>

<p>That reasoners will scale to solving interesting problems seems likely since such problems are verifiable (using autonomous proof checkers). Some disagree (here’s a relevant LessWrong post), taking the position that solving open problems is fundamentally different to solving closed problems. LLMs, they argue, have the latent ability to solve these problems because the knowledge is present in the data they’re trained on. And, they argue, the reinforcement learning process by which inference-time reasoners are trained simply elicits this latent ability—but since no such latent ability exists when it comes to solving novel problems, there’s a fundamental difference between the two, and we have no reason to suppose that reasoners will scale to solving novel problems any time soon.</p>

<p>But this is just a matter of how one chooses to carve out nature. Just as the techniques required to solve closed problems are available in the corpora of human data on which LLMs are trained, so too are the more abstract techniques required to solve open problems. The main distinction between the two is not some fundamental difference in nature, but rather the time horizon over which each occurs. Applying the techniques required to solve closed problems takes minutes or hours. Applying the techniques required to solve open problems takes days or weeks (and perhaps up to, effectively, indefinitely longer).</p>

<p>This chart describes which mathematics tasks AI can solve in terms of how long the task takes a skilled human to do. It’s based on extrapolative research by METR.</p>

<img src="math-chart.webp" style="max-width: 100%; height: auto; display: block; margin: 20px 0;">

<p>The fact that LLMs cannot yet apply the time-consuming techniques required to solve open problems is in fact largely uninformative given the time-horizon focused perspective which seems correct, and the informative evidence we do have (that the techniques which are within current-AI’s time horizons are successfully carried out by AI) suggests that we should expect that AI will be able to successfully carry out techniques which are within future-AI’s time horizons—assuming they’re not fundamentally different in kind.</p>

<h2>Posing Interesting Problems</h2>

<p>The ability to solve open questions is not sufficient for fully autonomous research. The reasoner must also be able to self-direct—to pose problems which are at once interesting and reasonably likely to be within the its reach to solve.</p>

<p>One might ask: “Can we not simply use the argument we just used to demonstrate that reasoners will scale to solve open questions, to demonstrate also that they will scale to be able to pose problems? Is this too not just a matter of time-horizons?”. Unfortunately, we cannot. The problems which reasoners have been able to solve so far are verifiable tasks. Solving open problems is also a verifiable task, so is not of a fundamentally different kind, which is why our argument above held. But we have yet to demonstrate that self-direction in research is a task which we will be able to make verifiable. So, because we have not established that posing problems does not belong to that class of tasks at which today’s reasoning models do not perform well, we have more work to do yet.</p>

<p>I’m going to break down what solving open problems will involve, and show that the techniques involved include a limited kind of self-direction in finding problems to solve, and that by transitive property this limited self-direction is a verifiable task. Then I will show that this limited self-direction can be used as a source of synthetic data on which models can be trained, enabling indefinite, fully self-directed research.</p>

<p>Solving open problems requires novel insight. Novel insight realistically requires trial-and-error: one comes up with an idea and sees if it goes anywhere. The AI is coming up with various potential novel ideas, and, assuming that LLMs of this scale are not fundamentally incapable of this kind of skill, should form a strong intuition about what kind of ideas tend to lead to successfully solving problems. Now, this may be a kind of self-direction, but it’s not self-directed problem-posing. So, we’re not where we want to be quite yet.</p>

<p>But most open problems are more difficult than this. One can’t just come up with a single idea that gets one straight to the solution. Rather, one has to make progress incrementally. Find some small property which one didn’t notice in one’s initial attempt to solve the problem. Play around with the implications. Try to solve the problem again, make some incremental progress. Find some more properties. Some more implications. Gradually, by selecting the right sub-problems to work on, one works toward solving the given problem. This process requires an understanding of how to find the kinds of problems are within one’s reach, and how to find problems which yield interesting implications. This is self-directed problem-posing, as an instrumental good to a verifiable task. So, by transitive property, this kind of self-directed problem-posing is a verifiable task.</p>

<p>But this is still just limited self-directed problem-posing. Yes, once the AI has a difficult problem given to it, it can do its best to pose problems the solutions to which will be instrumentally useful to solving the given problem. But it still requires a human-in-the-loop supplying the AI with its overall research direction.</p>

<p>But the problem-solving model’s reasoning traces are synthetic data which describe the process of finding problems to solve which are within the reasoner’s reach and which have useful implications. This synthetic data can be used to train a reasoner to continuously pose and solve interesting problems. The details of this are left as an exercise for the reader.</p>

<p>So, I have presented two arguments. One that reasoners will scale to solving open problems, and one that the reasoning traces can be used to train reasoners which autonomously pose novel problems. This kind of reasoner will be a fully-autonomous mathematics researcher.</p>
                </div>
            </article>
            
            <article class="article-view" id="near-future-fiction">
                <h2>Near-Future Fiction</h2>
                <p class="article-date">10 Oct 2024</p>
                <div class="content">
                    <p>In 2027 the trend that began in 2024 with OpenAI's o1 reasoning system has continued. The compute required to run AI is no longer negligible compared to the cost of training it. Models reason over long periods of time. Their effective context windows are massive, they update their underlying models continuously, and they break tasks down into sub-tasks to be carried out in parallel. The base LLM they are built on is two generations ahead of GPT-4.</p>

<p>These systems are language model agents. They are built with self-understanding and can be configured for autonomy. These constitute proto-AGI. They are artificial intelligences that can perform much but not all of the intellectual work that humans can do (although even what these AI can do, they cannot necessarily do cheaper than a human could).</p>

<p>In 2029 people have spent over a year working hard to improve the scaffolding around proto-AGI to make it as useful as possible. Presently, the next generation of LLM foundational model is released. Now, with some further improvements to the reasoning and learning scaffolding, this is true AGI. It can perform any intellectual task that a human could (although it's very expensive to run at full capacity). It is better at AI research than any human. But it is not superintelligence. It is still controllable and its thoughts are still legible. So, it is put to work on AI safety research. Of course, by this point much progress has already been made on AI safety - but it seems prudent to get the AGI to look into the problem and get its go-ahead before commencing with the next training run. After a few months the AI declares it has found an acceptable safety approach. It spends some time on capabilities research then the training run for the next LLM begins.</p>

<p>In 2030 the next LLM is completed, and improved scaffolding is constructed. Now human-level AI is cheap, better-than-human-AI is not too expensive, and the peak capabilities of the AI are almost alien. For a brief period of time the value of human labour skyrockets, workers acting as puppets as the AI instructs them over video-call to do its bidding. This is necessary due to a major robotics shortfall. Human puppet-workers work in mines, refineries, smelters, and factories, as well as in logistics, optics, and general infrastructure. Human bottlenecks need to be addressed. This takes a few months, but the ensuing robotics explosion is rapid and massive.</p>

<p>2031 is the year of the robotics explosion. The robots are physically optimised for their specific tasks, coordinate perfectly with other robots, are able to sustain peak performance, do not require pay, and are controlled by cleverer-than-human minds. These are all multiplicative factors for the robots' productivity relative to human workers. Most robots are not humanoid, but let's say a humanoid robot would cost $x. Per $x robots in 2031 are 10,000 more productive than a human. This might sound like a ridiculously high number: one robot the equivalent of 10,000 humans? But let's do some rough math:</p>

<ul>
    <li>Physically optimised for their specific tasks: <strong>5x</strong></li>
    <li>Coordinate perfectly with other robots: <strong>10x</strong></li>
    <li>Able to sustain peak performance: <strong>5x</strong></li>
    <li>Do not require pay: <strong>2x</strong></li>
    <li>Controlled by cleverer-than-human minds: <strong>20x</strong></li>
    <li><strong>Total Multiplier: 5 * 10 * 5 * 2 * 20 = 10,000</strong></li>
</ul>

<p>Suppose that a human can construct one robot per year (taking into account mining and all the intermediary logistics and manufacturing). With robots 10^4 times as productive as humans, each robot will construct an average of 10^4 robots per year. This is the robotics explosion. By the end of the year there will be a 10^11 robots (more precisely, an amount of robots that is cost-equivalent to 10^11 humanoid robots).</p>

<p>By 2032 there are 10^11 robots, each with the productivity of 10^4 skilled human workers. That is a total productivity equivalent to 10^15 skilled human workers. This is roughly 10^5 times the productivity of humanity in 2024. At this point trillions of advanced processing units have been constructed and are online. Industry expands through the Solar System. The number of robots continues to balloon. The rate of research and development accelerates rapidly. Human mind upload is achieved.</p>
                </div>
            </article>
            
        </main>

        <script>
         function toggleTheme() {
             document.body.classList.toggle('dark-mode');
             const isDark = document.body.classList.contains('dark-mode');
             localStorage.setItem('theme', isDark ? 'dark' : 'light');
         }

         function initTheme() {
             const savedTheme = localStorage.getItem('theme');
             const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
             
             if (savedTheme === 'dark' || (!savedTheme && prefersDark)) {
                 document.body.classList.add('dark-mode');
             }
         }

         initTheme();

         function showArticle(id) {
             document.querySelector('.home-view').classList.add('hidden');
             document.querySelectorAll('.article-view').forEach(el => el.classList.remove('active'));
             document.getElementById(id).classList.add('active');
             history.pushState(null, '', '#' + id);
             window.scrollTo(0, 0);
         }

         function showHome() {
             document.querySelectorAll('.article-view').forEach(el => el.classList.remove('active'));
             document.querySelector('.home-view').classList.remove('hidden');
             history.pushState(null, '', window.location.pathname);
             window.scrollTo(0, 0);
         }

         function handleHash() {
             const hash = window.location.hash.slice(1);
             if (hash && document.getElementById(hash)) {
                 document.querySelector('.home-view').classList.add('hidden');
                 document.querySelectorAll('.article-view').forEach(el => el.classList.remove('active'));
                 document.getElementById(hash).classList.add('active');
             } else {
                 document.querySelectorAll('.article-view').forEach(el => el.classList.remove('active'));
                 document.querySelector('.home-view').classList.remove('hidden');
             }
         }

         window.addEventListener('hashchange', handleHash);
         window.addEventListener('load', handleHash);
        </script>
    </body>
</html>